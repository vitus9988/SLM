name: llama.cpp server + lm_eval benchmark

on:
  workflow_dispatch:
  push:
    branches: [ main ]

jobs:
  benchmark:
    runs-on: ubuntu-latest
    env:
      HF_TOKEN: ${{ secrets.HF_TOKEN }}

    steps:
    - uses: actions/checkout@v4

    - uses: actions/setup-python@v5
      with:
        python-version: "3.11"

    - name: Install system packages
      run: |
        sudo apt-get update
        sudo apt-get install -y \
          git build-essential pkg-config libopenblas-dev jq cmake
        cmake --version
         

    - name: Install Python dependencies
      run: |
        python -m pip install -U pip setuptools wheel
        CMAKE_ARGS="-DGGML_BLAS=ON -DGGML_BLAS_VENDOR=OpenBLAS" \
        pip install 'llama-cpp-python[server]'
        git clone --depth 1 https://github.com/EleutherAI/lm-evaluation-harness.git
        pip install -e ./lm-evaluation-harness
        pip install --quiet "huggingface_hub[cli]"
  
    - name: Download Gemma model
      run: |
        mkdir -p models
        huggingface-cli download ggml-org/gemma-3-1b-it-GGUF \
          gemma-3-1b-it-Q4_K_M.gguf \
          --local-dir ./models \
          --local-dir-use-symlinks False
        ls -lh models


    - name: Launch server & run lm_eval
      run: |
        MODEL_PATH="models/gemma-3-1b-it-Q4_K_M.gguf"

        python -m llama_cpp.server --model "$MODEL_PATH"

        SERVER_PID=$!
        echo "Server PID: $SERVER_PID"

        # ② 서버 기동 대기 (최대 60초)
        for i in {1..12}; do
          if curl -s http://127.0.0.1:8000/ -o /dev/null; then
            echo "✅ llama.cpp server is up"; break
          fi
          echo "⏳ waiting for server... ($i/12)"; sleep 5
        done

        # ③ kobest_boolq 벤치마크 실행
        lm_eval \
          --model gguf \
          --model_args base_url=http://localhost:8000 \
          --tasks kobest_boolq \
          --batch_size auto

        EXIT_CODE=$?

        # ④ 서버 종료
        kill $SERVER_PID
        wait $SERVER_PID || true

        exit $EXIT_CODE
